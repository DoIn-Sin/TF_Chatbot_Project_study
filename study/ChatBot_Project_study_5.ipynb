{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 모델\n",
    "### CNN : 챗봇 엔진에 문장 의도 분류를 위해 사용한다. 합성곱 신경망이라 불리며 이미지 분류에 좋은 성능을 가지고 있어 최근에 자율주행 자동차, 얼굴인식 등 이미지 판별 분야에서 많이 사용한다.\n",
    "#### 합성곱 : 합성곱 필터라고 불리는 특정 크기의 행렬을 이미지 데이터(or 문장 데이터) 행렬에 슬라이딩 하면서 곱하고 더하는 연산을 말한다. 합성곱 연산은 필터가 더 이상 슬라이딩 할 수 없을 때까지 반복한다. 합성곱 필터는 다른 말로 마스크, 윈도우, 커널 등으로 불린다.\n",
    "#### 특징맵 : 합성곱 연산을 통해 최종적으로 나온 결과를 말한다.\n",
    "#### 패딩 : 출력의 크기를 조정하며 패딩된 곳은 0으로 채워진다.\n",
    "#### 풀링 : 합성곱 연산 결과로 나온 특징맵의 크기를 줄이거나 주요한 특징을 추출하기 위해 사용하는 연산이다. 풀링 연산에는 최대 풀링과 평균 풀링 연산이 있으며 주로 최대 풀링 연산을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 문답 데이터 감정 분류 모델 by CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "train_file = \"./ChatbotData .csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 인덱스 시퀀스 벡터\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련셋, 테스트셋, 검증셋 생성\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "\n",
    "train_size = int(len(padded_seqs)*0.7)\n",
    "val_size = int(len(padded_seqs)*0.2)\n",
    "test_size = int(len(padded_seqs)*0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size+val_size).take(test_size).batch(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 값 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(word_index)+1 # 전체 단어 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 정의\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(\n",
    "    filters=128, \n",
    "    kernel_size=3, \n",
    "    padding='valid', \n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(\n",
    "    filters=128, \n",
    "    kernel_size=4, \n",
    "    padding='valid', \n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(\n",
    "    filters=128, \n",
    "    kernel_size=5, \n",
    "    padding='valid', \n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3, 4, 5-gram 이후 합치기\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(3, name='logits')(dropout_hidden)\n",
    "predictions = Dense(3, activation=tf.nn.softmax)(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "414/414 [==============================] - 4s 10ms/step - loss: 0.8948 - accuracy: 0.5615 - val_loss: 0.5413 - val_accuracy: 0.8139\n",
      "Epoch 2/5\n",
      "414/414 [==============================] - 4s 9ms/step - loss: 0.5249 - accuracy: 0.8034 - val_loss: 0.2739 - val_accuracy: 0.9150\n",
      "Epoch 3/5\n",
      "414/414 [==============================] - 4s 9ms/step - loss: 0.2996 - accuracy: 0.8996 - val_loss: 0.1242 - val_accuracy: 0.9602\n",
      "Epoch 4/5\n",
      "414/414 [==============================] - 4s 9ms/step - loss: 0.1884 - accuracy: 0.9415 - val_loss: 0.1019 - val_accuracy: 0.9674\n",
      "Epoch 5/5\n",
      "414/414 [==============================] - 4s 9ms/step - loss: 0.1361 - accuracy: 0.9584 - val_loss: 0.0583 - val_accuracy: 0.9814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f4c3c39be0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 1ms/step - loss: 0.0590 - accuracy: 0.9856\n",
      "Accuracy: 98.561758\n",
      "loss: 0.058960\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print('Accuracy: %f'%(accuracy*100))\n",
    "print('loss: %f'%(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 무답 데이터 감정 분류 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "train_file = \"./ChatbotData .csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 인덱스 시퀀스 벡터\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋 생성\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "test_ds = ds.take(2000).batch(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 15, 128)      1715072     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 15, 128)      0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 13, 128)      49280       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 12, 128)      65664       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 11, 128)      82048       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          49280       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "logits (Dense)                  (None, 3)            387         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            12          logits[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,961,743\n",
      "Trainable params: 1,961,743\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "100/100 - 0s - loss: 0.0565 - accuracy: 0.4405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05648089945316315, 0.4404999911785126]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정 분류 CNN 모델 불러오기\n",
    "model = load_model('cnn_model.h5')\n",
    "model.summary()\n",
    "model.evaluate(test_ds, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 시퀀스 :  ['썸', '타는', '여자가', '남사친', '만나러', '간다는데', '뭐라', '해']\n",
      "단어 인덱스 시퀀스 :  [   13    61   127  4320  1333 12162   856    31     0     0     0     0\n",
      "     0     0     0]\n",
      "문장 분류(정답) :  2\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋 10212번째 데이터 출력\n",
    "print('단어 시퀀스 : ', corpus[10212])\n",
    "print('단어 인덱스 시퀀스 : ', padded_seqs[10212])\n",
    "print('문장 분류(정답) : ', labels[10212])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정 예측 점수 :  [[1.5855716e-06 9.8004894e-06 9.9998856e-01]]\n",
      "감정 예측 클라스 :  [2]\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋의 10212번째 데이터 감정 예측\n",
    "picks = [10212]\n",
    "predict = model.predict(padded_seqs[picks])\n",
    "predict_class = tf.math.argmax(predict, axis=1)\n",
    "print('감정 예측 점수 : ', predict)\n",
    "print('감정 예측 클라스 : ', predict_class.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
